name: Upload Reddit Data to Airtable

on:
  workflow_dispatch:

jobs:
  upload:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout
      - uses: actions/checkout@v4

      # 2. Download uploaded artifact from previous scrape run
      - name: Download scrape result
        uses: actions/download-artifact@v4
        with:
          name: reddit_scrape_output
          path: ./data

      # 3. Unzip
      - run: unzip ./data/reddit_scrape_output.zip -d ./data

      # 4. Set up Python
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - run: pip install requests

      # 5. Push to Airtable
      - name: Upload to Airtable
        env:
          AIRTABLE_API_KEY: ${{ secrets.AIRTABLE_API_KEY }}
          AIRTABLE_BASE_ID: ${{ secrets.AIRTABLE_BASE_ID }}
        run: |
          python <<'PY'
          import os, json, requests
          with open('data/output.json', encoding='utf-8') as f:
              posts = json.load(f)

          url = f"https://api.airtable.com/v0/{os.environ['AIRTABLE_BASE_ID']}/Problems"
          headers = {
              "Authorization": f"Bearer {os.environ['AIRTABLE_API_KEY']}",
              "Content-Type": "application/json"
          }

          sent = 0
          for p in posts:
              rec = {
                "fields": {
                  "Name":   p["post"]["title"][:255],
                  "Notes":  p["post"]["selftext"][:800],
                  "Lang":   "DE" if p["post"]["subreddit"] in ("de_IT","selfhosted","StartupDACH") else "EN",
                  "Tags":   [p["post"]["subreddit"]]
                }
              }
              r = requests.post(url, headers=headers, json=rec)
              if r.status_code == 200: sent += 1
              else: print("❌", r.status_code, r.text)
          print(f"✅ Uploaded {sent}/{len(posts)} records")
          PY
