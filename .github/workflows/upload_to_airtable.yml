name: Upload Reddit Data to Airtable

on:
  workflow_dispatch:

jobs:
  upload:
    runs-on: ubuntu-latest
    steps:
      # 1â€†Checkout
      - uses: actions/checkout@v4

      # 2â€†Download artifact from the latest successful run of reddit_scrape.yml
      - name: Download scrape result
        uses: actions/download-artifact@v4
        with:
          name: reddit_scrape_output
          path: ./data
          workflow: reddit_scrape.yml        # ðŸ‘ˆ tells GH to look at that workflow
          workflow_conclusion: success

      # 3â€†Unzip
      - run: unzip ./data/reddit_scrape_output.zip -d ./data

      # 4â€†Python
      - uses: actions/setup-python@v5
        with: {python-version: '3.11'}

      - run: pip install requests

      # 5â€†Push to Airtable
      - name: Upload to Airtable
        env:
          AIRTABLE_API_KEY: ${{ secrets.AIRTABLE_API_KEY }}
          AIRTABLE_BASE_ID: ${{ secrets.AIRTABLE_BASE_ID }}
        run: |
          python <<'PY'
          import os, json, requests
          with open('data/output.json', encoding='utf-8') as f:
              posts = json.load(f)

          url = f"https://api.airtable.com/v0/{os.environ['AIRTABLE_BASE_ID']}/Problems"
          headers = {"Authorization": f"Bearer {os.environ['AIRTABLE_API_KEY']}",
                     "Content-Type": "application/json"}

          sent = 0
          for p in posts:
              rec = {
                "fields": {
                  "Name":   p["post"]["title"][:255],
                  "Notes":  p["post"]["selftext"][:800],
                  "Lang":   "DE" if p["post"]["subreddit"] in ("de_IT","selfhosted","StartupDACH") else "EN",
                  "Tags":   [p["post"]["subreddit"]]
                }
              }
              r = requests.post(url, headers=headers, json=rec)
              if r.status_code == 200: sent += 1
              else: print("âŒ", r.status_code, r.text)
          print(f"âœ… Uploaded {sent}/{len(posts)} records")
          PY
