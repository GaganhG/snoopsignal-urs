     __  __  _ __   ____  
    /\ \/\ \/\`'__\/',__\ 
    \ \ \_\ \ \ \//\__, `\
     \ \____/\ \_\\/\____/
      \/___/  \/_/ \/___/... Universal Reddit Scraper 

![GitHub top language](https://img.shields.io/github/languages/top/JosephLai241/URS?logo=Python)
[![PRAW Version](https://img.shields.io/badge/PRAW-7.0.0-red?logo=Reddit)][PRAW]
[![Build Status](https://img.shields.io/travis/JosephLai241/URS?logo=Travis)][Travis CI Build Status]
[![GitHub Workflow Status](https://img.shields.io/github/workflow/status/JosephLai241/URS/Pytest?logo=github)][Github Actions - Pytest]
[![Codecov](https://img.shields.io/codecov/c/gh/JosephLai241/URS?logo=Codecov)][Codecov]
[![GitHub release (latest by date)](https://img.shields.io/github/v/release/JosephLai241/URS)][Releases]
![License](https://img.shields.io/github/license/JosephLai241/URS)

[![Email](https://img.shields.io/badge/Email-urs__project%40protonmail.com-informational?logo=ProtonMail)][URS Project Email]
[![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-blue)][Say Thanks!]

<p align="center"> 
    <img src="https://github.com/JosephLai241/URS/blob/master/.github/Screenshots/Demo%20GIFs/DEMO.gif">
</p>

# Table of Contents

* [Introduction](#introduction)
* [URS Overview](#urs-overview)
    + [Getting Started](#getting-started)
    + [Table of All Subreddit, Redditor, and Submission Comments Attributes](#a-table-of-all-subreddit-redditor-and-submission-comments-attributes)
    + [Subreddits](#subreddits)
    + [Redditors](#redditors)
    + [Submission Comments](#submission-comments)
    + [Exporting](#exporting)
* [Contributing](#contributing)
    + [Before Making Pull or Feature Requests](#before-making-pull-or-feature-requests)
    + [Building on Top of URS](#building-on-top-of-urs)
    + [Making Pull or Feature Requests](#making-pull-or-feature-requests)
* [Contributors](#contributors)
* [Derivative Projects](#derivative-projects)
* [Releases](#releases)
* Supplemental Documents
    + [How to get Reddit API Credentials for PRAW][How to get Reddit API Credentials for PRAW]
    + [Error Messages and Rate Limit Information][Error Messages and Rate Limit Info]
    + [2-Factor Authentication][2-Factor Authentication]
    + [Some Linux Tips][Some Linux Tips]

# Introduction

This is a universal Reddit scraper that can scrape Subreddits, Redditors, and comments from submissions. 

Written in Python and utilizes the official Reddit API ([ `PRAW` ][PRAW]).

Run `pip install -r requirements.txt` to get all project dependencies. 

You will need your own Reddit account and API credentials for PRAW. See the [Getting Started](#getting-started) section for more information. 

***NOTE:*** `PRAW` is currently supported on Python 3.5+. This project was tested with Python 3.8.2. 

**Whether you are using URS for enterprise or personal use, I am very interested in hearing about your use cases and how it has helped you achieve a goal. Please send me an email or leave a note by clicking on the Email or Say Thanks! badge. I look forward to hearing from you!**

# URS Overview

## Scrape Speeds

Scrape speed is determined by a couple things:

* The number of results returned for Subreddit or Redditor scraping
* The submission's popularity (total number of comments) for submission comments scraping
* Your internet connection speed

## Export Directory Structure

All exported files are saved within the `scrapes` directory and stored in a sub-directory labeled with the date. These directories are automatically created when you run URS. 

This is an example directory structure generated by the [tree command][tree].

```
PASTE TREE OUTPUT HERE AFTER GETTING SAMPLES.
```

## Getting Started

It is very quick and easy to get Reddit API credentials. Refer to [my guide][How to get Reddit API Credentials for PRAW] to get your credentials, then update the `API` dictionary located in `Credentials.py`

## A Table of All Subreddit, Redditor, and Submission Comments Attributes

These attributes are included in each scrape. 

| Subreddits    | Redditors                      | Submission Comments |
|---------------|--------------------------------|---------------------|
| Title         | Name                           | Parent ID           |
| Flair         | Fullname                       | Comment ID          |
| Date Created  | ID                             | Author              |
| Upvotes       | Date Created                   | Date Created        |
| Upvote Ratio  | Comment Karma                  | Upvotes             |
| ID            | Link Karma                     | Text                |
| Is Locked?    | Is Employee?                   | Edited?             |
| NSFW?         | Is Friend?                     | Is Submitter?       |
| Is Spoiler?   | Is Mod?                        | Stickied?           |
| Stickied?     | Is Gold?                       |                     |
| URL           | \*Submissions                  |                     |
| Comment Count | \*Comments                     |                     |
| Text          | \*Hot                          |                     |
| &nbsp;        | \*New                          |                     |
| &nbsp;        | \*Controversial                |                     |
| &nbsp;        | \*Top                          |                     |
| &nbsp;        | \*Upvoted (may be forbidden)   |                     |
| &nbsp;        | \*Downvoted (may be forbidden) |                     |
| &nbsp;        | \*Gilded                       |                     |
| &nbsp;        | \*Gildings (may be forbidden)  |                     |
| &nbsp;        | \*Hidden (may be forbidden)    |                     |
| &nbsp;        | \*Saved (may be forbidden)     |                     |

\*Includes additional attributes; see [Redditors](#redditors) section for more information. 

## Subreddits

![Subreddit Demo GIF][Subreddit Demo]

\*This GIF is uncut.

**Usage:** `$ ./Urs.py -r SUBREDDIT [H|N|C|T|R|S] N_RESULTS_OR_KEYWORDS --FILE_FORMAT` 

You can specify Subreddits, the submission category, and how many results are returned from each scrape. I have also added a search option where you can search for keywords within a Subreddit.

These are the submission categories:

* Hot
* New
* Controversial
* Top
* Rising
* Search

The file names for all categories except for Search will follow this format: 

`"[SUBREDDIT]-[POST_CATEGORY]-[N_RESULTS]-result(s).[FILE_FORMAT]"`

If you searched for keywords, file names will follow this format:

`"[SUBREDDIT]-Search-'[KEYWORDS]'.[FILE_FORMAT]"` 

### Time Filters

Time filters may be applied to some categories. Here is a table of the categories on which you can apply a time filter as well as the valid time filters.

| Categories    | Time Filters  | 
|---------------|---------------|
| Controversial | All (default) |
| Search        | Day           |
| Top           | Hour          |
| &nbsp;        | Month         | 
| &nbsp;        | Week          |
| &nbsp;        | Year          |

Specify the time filter after the number of results returned or keywords you want to search for.

**Usage:** `$ ./Urs.py -r SUBREDDIT [C|T|S] N_RESULTS_OR_KEYWORDS OPTIONAL_TIME_FILTER --FILE_FORMAT`

If no time filter is specified, the default time filter `all` is applied. The Subreddit settings table will display `None` for categories that do not offer the additional time filter option.

If you specified a time filter, `-past-[TIME_FILTER]` will be appended to the file name before the file format like so: 

`"[SUBREDDIT]-[POST_CATEGORY]-[N_RESULTS]-result(s)-past-[TIME_FILTER].[FILE_FORMAT]"` 

Or if you searched for keywords:

`"[SUBREDDIT]-Search-'[KEYWORDS]'-past-[TIME_FILTER].[FILE_FORMAT]"`

Exported files will be saved to the `subreddits` directory.

***NOTE:*** Up to 100 results are returned if you search for something within a Subreddit. You will not be able to specify how many results to keep.

## Redditors

![Redditor Demo GIF][Redditor Demo]

\*This GIF has been cut for demonstration purposes.

**Usage:** `$ ./Urs.py -u USER N_RESULTS --FILE_FORMAT` 

**Designed for JSON only.**

You can also scrape Redditor profiles and specify how many results are returned.

Some Redditor attributes are sorted differently. Here is a table of how each is sorted.

| Attribute Name | Sorted By/Time Filter                       |
|----------------|---------------------------------------------|
| Comments       | Sorted By: New                              |
| Controversial  | Time Filter: All                            |
| Gilded         | Sorted By: New                              |
| Hot            | Determined by other Redditors' interactions |
| New            | Sorted By: New                              |
| Submissions    | Sorted By: New                              |
| Top            | Time Filter: All                            |

Of these Redditor attributes, the following will include additional attributes:

| Submissions, Hot, New, Controversial, Top, Upvoted, Downvoted, Gilded, Gildings, Hidden, and Saved | Comments                                     |
|----------------------------------------------------------------------------------------------------|----------------------------------------------|
| Title                                                                                              | Date Created                                 |
| Date Created                                                                                       | Score                                        |
| Upvotes                                                                                            | Text                                         |
| Upvote Ratio                                                                                       | Parent ID                                    |
| ID                                                                                                 | Link ID                                      |
| NSFW?                                                                                              | Edited?                                      |
| Text                                                                                               | Stickied?                                    |
| &nbsp;                                                                                             | Replying to (title of submission or comment) |
| &nbsp;                                                                                             | In Subreddit (Subreddit name)                |

The file names will follow this format: 

`"[USERNAME]-[N_RESULTS]-result(s).[FILE_FORMAT]"` 

Exported files will be saved to the `redditors` directory.

***NOTE:*** If you are not allowed to access a Redditor's lists, PRAW will raise a 403 HTTP Forbidden exception and the program will just append a "FORBIDDEN" underneath that section in the exported file. 

***NOTE:*** The number of results returned are applied to all attributes. I have not implemented code to allow users to specify different number of results returned for individual attributes. 

## Submission Comments

![Structured Comments Demo GIF][Structured Comments Demo]
![Raw Comments Demo GIF][Raw Comments Demo]

\*These GIFs have been cut for demonstration purposes.

**Usage:** `$ ./Urs.py -c URL N_RESULTS --FILE_FORMAT` 

**Designed for JSON only.**

You can also scrape comments from submissions and specify the number of results returned. 

Comments are sorted by "Best", which is the default sorting option when you visit a submission.

**There are two ways you can scrape comments: structured or raw.** This is determined by the number you pass into `N_RESULTS`:

| Scrape Type | N_RESULTS      |
|-------------|----------------|
| Structured  | N_RESULTS >= 1 |
| Raw         | N_RESULTS = 0  |

Structured scrapes resemble comment threads on Reddit and will include down to third-level comment replies. 

Raw scrapes do not resemble comment threads, but returns all comments on a submission in level order: all top-level comments are listed first, followed by all second-level comments, then third, etc.

Of all scrapers included in this program, this usually takes the longest to execute. PRAW returns submission comments in level order, which means scrape speeds are proportional to the submission's popularity.

The file names will follow this format: 

`"[POST_TITLE]-[N_RESULTS]-result(s).[FILE_FORMAT]"` 

Exported files will be saved to the `comments` directory.

***NOTE:*** You cannot specify the number of raw comments returned. The program with scrape all comments from the submission. 

## Exporting

URS supports exporting to either CSV or JSON.

Here are my recommendations for scrape exports.

| Scraper           | File Format |
|-------------------|-------------|
| Subreddit / Basic | CSV or JSON |
| Redditor          | JSON        |
| Comments          | JSON        |

Subreddit scrapes will work well with either format.

JSON is the more practical option for Redditor and submission comments scraping, which is why I have designed these scrapers to work best in this format. It is much easier to read the scrape results since Redditor scraping returns attributes that include additional submission or comment attributes. 

Comments scraping is especially easier to read in JSON format because structured exports look similar to threads on Reddit. You can process all the information pertaining to a comment much quicker compared to CSV. 

You can still export Redditor data and submission comments to CSV, but you will be disappointed with the results.

### See the [samples][Samples] for scrapes ran on February, TBD 2021.

# Contributing

**Please contact me at the email address listed in the email badge at the top of this README.**

## Before Making Pull or Feature Requests

Consider the scope of this project before submitting a pull or feature request. URS stands for Universal Reddit Scraper. Two important aspects are listed in its name - *universal* and *scraper*.

I will not approve feature or pull requests that deviate from its sole purpose. This may include scraping a specific aspect of Reddit or [adding functionality that allows you to post a comment with URS][Commenting Feature Request]. Adding either of these requests will no longer allow URS to be universal or merely a scraper. However, I am more than happy to approve requests that enhance the current scraping capabilities of URS.

## Building on Top of URS

Although I won't approve requests that deviate from the project scope, feel free to reach out if you've built something on top of URS or have made modifications to scrape something specific on Reddit. I will add your project to the Derivative Projects section!

## Making Pull or Feature Requests

You can suggest new features or changes by going to the [Issues tab][Issues] and fill out the Feature Request template. If there is a good reason for a new feature, I will consider adding it.

You are also more than welcome to create a pull request - adding additional features, improving runtime, or refactoring existing code. If it is approved, I will merge the pull request into the master branch and credit you for contributing to this project.

# Contributors

| Date           | User                                                      | Contribution                                                                                                               |
|----------------|-----------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|
| March 11, 2020 | [ThereGoesMySanity][ThereGoesMySanity] | Created a [pull request][ThereGoesMySanity Pull Request] adding 2FA information to README |
| October 6, 2020 | [LukeDSchenk][LukeDSchenk] | Created a [pull request][LukeDSchenk Pull Request] fixing "[Errno 36] File name too long" issue, making it impossible to save comment scrapes with long titles |
| October 10, 2020 | [IceBerge421][IceBerge421] | Created a [pull request][IceGerge421 Pull Request] fixing a cloning error occuring on Windows machines due to illegal filename characters, `"`, found in two scrape samples |

# Derivative Projects

This is a showcase for projects that are built on top of URS!

| Project Name | Author                 | Description |
|--------------|------------------------|-------------|
| &nbsp;       | [skiwheelr][skiwheelr] | &nbsp;      |

# Releases

| Release Date | Version | Changelog | 
|--------------|---------|-----------|
| **May 25, 2019** | URS v1.0.0 | <ul> <li>Its inception.</li> </ul> |
| **July 29, 2019** | URS v2.0.0 | <ul> <li>Now **includes CLI support**!</li> </ul> |
| **December 28, 2019** | URS v3.0.0 (beta) | <ul> <li>Added **JSON** export.</li> <li>Added **Redditor Scraping**.</li> <li>Comments scraping is still under construction.</li> </ul> | 
| **December 31, 2019** | URS v3.0.0 (Official) | <ul> <li>**Comments scraping is now working**!</li> <li>**Added additional exception handling** for creating filenames.</li> <li>Minor code reformatting.</li> <li>**Simplified verbose output**.</li> <li>**Added an additional submission attribute** when scraping Redditors.</li> <li>Happy New Year!</li> </ul> |
| **January 15, 2020** | URS v3.0.0 (Final Release) | <ul> <li>Numerous changes to `README`.</li> <li>Minor code reformatting.</li> <li>**Fulfilled community standards** by adding the following docs:</li> <ul> <li>[Contributing Guide][Contributing Guide]</li> <li>[Pull Request Template][Pull Request Template]</li> <li>Issue templates:</li> <ul> <li>[Bug Report][Bug Report]</li> <li>[Feature Request][Feature Request]</li> </ul> <li>[Code of Conduct][Code of Conduct]</li> <li>[License][License]</li> </ul> </ul> |
| **June 22, 2020** | URS v3.1.0 | <ul> <li>***Major*** code refactor. **Applied OOP concepts** to existing code and rewrote methods in attempt to **improve readability, maintenance, and scalability**.</li> <li>**New in 3.1.0**:</li> <ul> <li>**Scrapes will now be exported to the `scrapes/` directory** within a subdirectory corresponding to the date of the scrape. These directories are automatically created for you when you run URS.</li> <li>Added **log decorators** that record what is happening during each scrape, which scrapes were ran, and any errors that might arise during runtime in the log file `scrapes.log`. The log is stored in the same subdirectory corresponding to the date of the scrape.</li> <li>**Replaced bulky titles with minimalist titles** for a cleaner look.</li> <li>**Added color to terminal output**.</li> </ul> <li>**Improved naming convention** for scripts.</li> <li>Integrating **Travis CI** and **Codecov**.</li> <li>Updated community documents located in the `.github/` directory: `BUG_REPORT`, `CONTRIBUTING`, `FEATURE_REQUEST`, `PULL_REQUEST_TEMPLATE`, and `STYLE_GUIDE`</li> <li>Numerous changes to `README`. The most significant change was **splitting and storing walkthroughs in `docs/`**.</li> </ul> |
| **June 27, 2020** | URS v3.1.1 | <ul> <li>**Added time filters for Subreddit categories (Controversial, Search, Top)**.</li> <li>**Updated `README` to reflect new changes**.</li> <li>**Updated style guide**. Made **minor formatting changes to scripts** to reflect new rules.</li> <li>Performed **DRY code review**.</li> </ul> |
| **February TBD, 2021** | URS v3.1.2 | <ul> <li>**Scrapes will now be exported to sub-folders within the date directory.**</li> <ul> <li>`comments`, `redditors`, and `subreddits` directories are now created for you when you run each of the scrapers. Scrape results will now be stored within these directories.</li> </ul> <li>Minor code reformatting and refactoring.</li> <ul> <li>The forbidden access message that may appear when running the Redditor scraper is now yellow to avoid confusion.</li> </ul><li>Updated `README` and `STYLE_GUIDE`. Made a minor change to PRAW credentials guide.</li> <ul> <li>Added new Derivative Projects section.</li> </ul> </ul> 

<!-- BADGES: Links for the badges at the top of the README -->
[Codecov]: https://codecov.io/gh/JosephLai241/URS
[PRAW]: https://pypi.org/project/praw/
[Releases]: https://github.com/JosephLai241/URS/releases
[Say Thanks!]: https://saythanks.io/to/jlai24142%40gmail.com
[Travis CI Build Status]: https://travis-ci.com/github/JosephLai241/URS
[Github Actions - Pytest]: https://github.com/JosephLai241/URS/actions?query=workflow%3APytest
[URS Project Email]: mailto:urs_project@protonmail.

<!-- DEMO GIFS: Links to demo GIFS -->
[Main Demo]: https://github.com/JosephLai241/URS/blob/master/.github/Screenshots/Demo%20GIFs/DEMO.gif
[Subreddit Demo]: https://github.com/JosephLai241/URS/blob/master/.github/Screenshots/Demo%20GIFs/Subreddit_demo.gif
[Redditor Demo]: https://github.com/JosephLai241/URS/blob/master/.github/Screenshots/Demo%20GIFs/Redditor_demo.gif
[Structured Comments Demo]: https://github.com/JosephLai241/URS/blob/master/.github/Screenshots/Demo%20GIFs/Comments_structured_demo.gif
[Raw Comments Demo]: https://github.com/JosephLai241/URS/blob/master/.github/Screenshots/Demo%20GIFs/Comments_raw_demo.gif

<!-- GITHUB LINKS: Links around the URS repo on GitHub -->
[Issues]: https://github.com/JosephLai241/URS/issues
[Commenting Feature Request]: https://github.com/JosephLai241/URS/issues/17

<!-- SEPARATE DOCS: Links to documents located in the docs/ directory -->
[2-Factor Authentication]: https://github.com/JosephLai241/URS/blob/master/docs/Two-Factor%20Authentication.md
[Error Messages and Rate Limit Info]: https://github.com/JosephLai241/URS/blob/master/docs/Error%20Messages.md
[How to get Reddit API Credentials for PRAW]: https://github.com/JosephLai241/URS/blob/master/docs/How%20to%20Get%20PRAW%20Credentials.md
[Some Linux Tips]: https://www.linkhere.com/

<!-- SAMPLES: Links to the samples directory -->
[Samples]: https://github.com/JosephLai241/URS/tree/master/samples/scrapes/06-27-2020

<!-- COMMUNITY DOCS: Links to the community docs -->
[Bug Report]: https://github.com/JosephLai241/URS/blob/master/.github/ISSUE_TEMPLATE/BUG_REPORT.md
[Code of Conduct]: https://github.com/JosephLai241/URS/blob/master/.github/CODE_OF_CONDUCT.md
[Contributing Guide]: https://github.com/JosephLai241/URS/blob/master/.github/CONTRIBUTING.md
[Feature Request]: https://github.com/JosephLai241/URS/blob/master/.github/ISSUE_TEMPLATE/FEATURE_REQUEST.md
[License]: https://github.com/JosephLai241/URS/blob/master/LICENSE
[Pull Request Template]: https://github.com/JosephLai241/URS/blob/master/.github/PULL_REQUEST_TEMPLATE.md

<!-- ThereGoesMySanity: Links for user ThereGoesMySanity and pull request -->
[ThereGoesMySanity]: https://github.com/ThereGoesMySanity
[ThereGoesMySanity Pull Request]: https://github.com/JosephLai241/URS/pull/9

<!-- LukeDSchenk: Links for user LukeDSchenk and pull request -->
[LukeDSchenk]: https://github.com/LukeDSchenk
[LukeDSchenk Pull Request]: https://github.com/JosephLai241/URS/pull/19

<!-- IceBerge421: Links for user IceBerge421 and pull request -->
[IceBerge421]: https://github.com/IceBerge421
[IceGerge421 Pull Request]: https://github.com/JosephLai241/URS/pull/20

<!-- DERIVATIVE PROJECTS LINKS: Links to projects that were built on top of URS -->
[skiwheelr]: https://github.com/skiwheelr
[skiwheelr Project Link]: https://github.com/skiwheelr/URS

<!-- ADDITIONAL LINKS: A space for useful links -->
[tree]: http://mama.indstate.edu/users/ice/tree
